
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <!--meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1"-->
	<br>
    <title>LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation</title>

    <!-- CSS includes -->
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
    <link href="mainpage.css" rel="stylesheet">
</head>
<body>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
	</meta>
<div id="header" class="container-fluid">
    <div class="row" style="text-align:center;padding:0;margin:0">
        <h1>LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation</h1>
        <div class="authors">
            <p>
			<a style="font-size:20px" href="https://koutilya-pnvr.github.io" target="new">Koutilya PNVR<sup>&dagger;</sup></a>
            &emsp;
			&emsp;
            <a style="font-size:20px" href="https://www.linkedin.com/in/bharat-singh-183b46164?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3B4QaKeY5tTjqipgztSZ%2B33w%3D%3D" target="new">Bharat Singh<sup>&Dagger;</sup></a>
			&emsp;
			&emsp;
            <a style="font-size:20px" href="https://pallabig.github.io/PallabiGhosh/" target="new">Pallabi Ghosh<sup>&sect;</sup></a>
			&emsp;
			&emsp;
            <a style="font-size:20px" href="https://behjat.github.io/" target="new">Behjat Siddiquie<sup>&sect;</sup></a>
            &emsp;
			&emsp;
            <a style="font-size:20px" href="http://www.cs.umd.edu/~djacobs/" target="new">David Jacobs<sup>&dagger;</sup></a>
            <br>
            <span style="font-size:20px"><sup>&dagger;</sup>University of Maryland College Park </span> 
			&emsp;
			&emsp; 
			<span style="font-size:20px"><sup>&Dagger;</sup>Vchar.ai </span>
			&emsp;
			&emsp; 
			<span style="font-size:20px"><sup>&sect;</sup>Amazon </span>
			</p>
<!--            <div style="text-align:center;color:#A40;margin-bottom:10;font-size: 20px;">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020</div>-->
   	  </div>

  </div>
</div>

<div class="container">
	<table align="center" width="100%">
        <tbody><tr>
            <td align="center" width="100%">
                <center>
<!--                    <span style="font-size:22px"><a href="https://github.com/koutilya-pnvr/SharinGAN">Code</a></span>    -->
<!--                    <span style="font-size:22px"><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/PNVR_SharinGAN_Combining_Synthetic_and_Real_Data_for_Unsupervised_Geometry_Estimation_CVPR_2020_paper.html">Paper [CVPR 2020] </a> </span>    -->
                    <span style="font-size:30px"><a href="https://arxiv.org/abs/2303.12343">Paper [Arxiv]</a> &nbsp &nbsp <a href="https://github.com/koutilya-pnvr/LD-ZNet">Code (coming soon)</a> &nbsp &nbsp <a href="https://koutilya-pnvr.github.io/LD-ZNet/">AIGI dataset (coming soon)</a></span> 
<!--					<a href="https://drive.google.com/drive/folders/1oZDJu5Y7nqN23Fcb6kCvXy1Do69l_YkQ?usp=sharing">AIGI dataset (coming soon)</a>-->
				</center>
            </td>
        </tr>
    </tbody></table>
</div>
	
<div class="container">
    <h2>Abstract</h2>
We present a technique for segmenting real and AI-generated images using latent diffusion models (LDMs) trained on internet-scale datasets. First, we show that the latent space of LDMs (z-space) is a better input representation compared to other feature representations like RGB images or CLIP encodings for text-based image segmentation. By training the segmentation models on the latent z-space, which creates a compressed representation across several domains like different forms of art, cartoons, illustrations, and photographs, we are also able to bridge the domain gap between real and AI-generated images. We show that the internal features of LDMs contain rich semantic information and present a technique in the form of LD-ZNet to further boost the performance of text-based segmentation. Overall, we show up to 6% improvement over standard baselines for text-to-image segmentation on natural images. For AI-generated imagery, we show close to 20% improvement compared to state-of-the-art techniques.
	
<div>
	<center><h2>Text-Based Image Segmentation</h2></center>
       <center>
		   <table align=center width="100%"><!--1000px-->
            <tr>
                <td width="25%"><!--600px-->
                  <center>
					  <h3>AI Generated Images</h3>
                      <a href="./assets/Results_AI.pdf" target="new"><img src = "./assets/Results_AI.png" width="100%"></img></href></a><br><!--500px-->
				  </center>
                </td>
				<td width="25%"><!--600px-->
					  <center>
						  <h3>Real Images</h3>
                      <a href="./assets/Results_Phrasecut.pdf" target="new"><img src = "./assets/Results_Phrasecut.png" width="90%"></img></href></a><br><br><br><!--500px-->
						  <h3>Multi-Category Segmentation</h3>
                      <a href="./assets/Results_Multi_segmentation.pdf" target="new"><img src = "./assets/Results_Multi_segmentation.png" width="90%"></img></href></a><br><br><br><!--500px-->
					<a href="./assets/Results_Multi_segmentation2.pdf" target="new"><img src = "./assets/Results_Multi_segmentation2.png" width="90%"></img></href></a><br><!--500px-->
				  </center>
                </td>
            </tr>
         </table>
</center>
</div>

<div>
	<table align=center width="100%">
			<tr>
				<td width="100%"><!--600px-->
                  <center>
					  <br><br>
					  <h2>SEEM vs LD-ZNet</h2>
                  		<a href="./assets/Comparision_LD-ZNet_SEEM.pdf" target = "new"><img src = "./assets/Comparision_LD-ZNet_SEEM.png" width="100%"></img></href></a><br><!--1000px-->
<!--						<i>Figure 2: Overview of the proposed architecture.</i>-->
                </center>
				<p>
					<br>
				</p>
                </td>
			</tr>

			<tr>
                <td width="75%"><!--600px-->
                  <center>
					  <br><br>
					  <h2>Motivation</h2>
					  <br>
                      <a href="./assets/Motivation_new1.pdf" target="new"><img src = "./assets/Motivation_new1.png" width="50%"></img></href></a><br><!--500px-->		
					<br>
                      <i>Figure 1: Coarse segmentation results from an LDM for two distinct images, demonstrating the encoding of fine-grained object-level semantic information within the model’s internal features.</i><br><br>
				  </center>
					<p>Teaching networks to accurately find the boundaries of objects is hard and at the same time annotation of boundaries at internet scale is impractical. Also, most self-supervised or weakly supervised problems do not incentivize learning boundaries. For example, training on classification or captioning allows models to learn the most discriminative parts of the image without focusing on boundaries. Our insight is that Latent Diffusion Models (LDMs), which can be trained without object level supervision at internet scale, must attend to object boundaries, and so we hypothesize that they can learn features which would be useful for text-based image segmentation. <br><br>
					
					To test the aforementioned hypothesis about the presence of object-level semantic information inside a pretrained LDM, we conduct a simple experiment. We compute the pixel-wise norm between the unconditional and text-conditional noise estimates from a pretrained LDM as part of the reverse diffusion process. This computation identifies the spatial locations that need to be modified for the noised input to align better with the corresponding text condition. Hence, the magnitude of the pixel-wise norm depicts regions that identify the text prompt. As shown in the Figure 1, the pixel-wise norm represents a coarse segmentation of the subject although the LDM is not trained on this task. This clearly demonstrates that these large scale LDMs can not only generate visually pleasing images, but their internal representations encode fine-grained semantic information, that can be useful for tasks like segmentation. </p>
                </td>
            </tr>

            <tr>
                <td width="100%"><!--600px-->
                  <center>
					  <h2>Method</h2>
                  		<a href="./assets/LDZNet-Summary.pdf" target = "new"><img src = "./assets/LDZNet-Summary.png" width="100%"></img></href></a><br><!--1000px-->
						<i>Figure 2: Overview of the proposed architecture.</i>
                </center>
				<p>
					
				</p>
                </td>
            </tr>
  </table>
</div>



<div class="container" >
	
	<center>
		<br><h2><strong>AIGI</strong> (<strong>AI G</strong>enerated <strong>I</strong>mages) Dataset</h2><br>
		<a href="./assets/AIGI_dataset.pdf" target = "new"><img src = "./assets/AIGI_dataset.png" width="100%"></img></href></a><br><!--1000px-->
						<i>Figure 3: Examples from the AIGI dataset with annotations.</i>
	</center>
<br>
	<p>We create a dataset consisting of AI-generated images which we name <strong>AIGI</strong> dataset, to showcase the usefulness of our approach for text-based segmentation on a different domain. We use 100 AI-generated images from <a href="https://lexica.art/">lexica.art</a> and manually annotated multiple regions for 214 text-prompts relevant to these images, as shown in Figure 3 above.</p>
	
<p>The AIGI dataset is made public here: <a href="https://drive.google.com/drive/folders/1oZDJu5Y7nqN23Fcb6kCvXy1Do69l_YkQ?usp=sharing">Google Drive Link</a></p>
</div>

<div class="container" >
	
	<center>
<!--		<br><h1>Results</h1><br>-->
		<br><h2>Quantitave Results for Text-Based Segmentation</h2>
  </center>
    <div>
			 <table align=center width="100%"><!--1000px-->
            	<tr>
					<td width="50%"><!--600px-->
					  <center>
						  <h3>Phrasecut test dataset</h3>
						<a><img src="./assets/Quantitative_Real.png" width="90%"></img></a><!--1000px-->
						<br>
						<i> Table 1: Text-based image segmentation performance on the PhraseCut testset. The performance of ZNet and LD-ZNet is highlighted in gray. Both these models outperform the baseline RGBNet on all the metrics.</i>
					 </center>
				  </td>
			 
					<td width="50%">
					<center>
						<h3>AIGI dataset</h3>
						<a><img src="./assets/Quantitative_AIGI.png" width="75%"></img></a><!--1000px-->
						<br>
						<i>Table 2: Generalization of the proposed LD-ZNet on our AIGI dataset when compared with other state-of-the-art text-based segmentation methods.</i>
					</center>
					</td>
				</tr>
			</table>
			<br>
            <span class="abstract"></span><br>        
  </div>
</div> 

<div class="container" >
	
  <center>
		<h2>More Qualitative Results of LD-ZNet</h2>
		<table align=center width="100%"><!--1000px-->
            	<tr>
				  <td width="50%"><!--600px-->
					  <center>
						  <h3>Online Real Images</h3>
						<a><img src="./assets/Results_use_of_LDM_features.png" width="90%"></img></a>
					  <br>
					  
					  <i>Figure 4: More qualitative examples where RGBNet fails to localize {``Guitar", ``Panda"} from animation images (top two rows), famous celebrities {``Scarlett Johansson", ``Kate Middleton"} (middle two rows) and objects such as {``Lamp", ``Trees"} from illustrations (bottom two rows). LD-ZNet benefits from using z combined with the internal LDM features to correctly segment these text prompts.</i>
					 
					 </td>
			 
				  <td width="50%">
					<center>
						<h3>AIGI dataset</h3>
						<a href="./assets/Results_AI2.pdf" target = "new"><img src = "./assets/Results_AI2.png" width="100%"></img></href></a><br>
					<br>
					<i>Figure 5: More qualitative results of LD-ZNet from AIGI dataset.</i>
					</center>
					</td>
				</tr>
	</table>
	<br>
  </center>
	<span class="abstract">We show that LD-ZNet works remarkably well on other domains like cartoons/illustrations. Figure 4 demonstrates some specific cases where RGBNet fails to segment or poorly segments the object being referred to, where as LD-ZNet segments the objects better. Figure 5 depicts LD-ZNet can perform accurate segmentation for text prompts which include cartoons (Pikachu, Godzilla), celebrities (Donald Trump, Spiderman), famous landmarks (Eiffel Tower).</span><br>        

</div> 

<div class="container">
        <h2>Bibtex citation</h2>
<!--        <p><span style="font-size:20px"><a href="https://arxiv.org/abs/2006.04026" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true" style="font-size:24px; margin:10px;"></i>[Paper Arxiv]</a>, to appear in CVPR 2020</span></p>-->
<pre class="citation">
@misc{pnvr2023ldznet,
      title={LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation}, 
      author={Koutilya Pnvr and Bharat Singh and Pallabi Ghosh and Behjat Siddiquie and David Jacobs},
      year={2023},
      eprint={2303.12343},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
</pre>
</div>

<div class="container" >
  <h2>Contact</h2>
  <div>
  <a href="mailto:koutilya@terpmail.umd.edu">Koutilya PNVR</a>
  </div>
</div>
<br><br><br>

<div id="footer">
</div>

<!-- Javascript includes -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>


</body></html>
